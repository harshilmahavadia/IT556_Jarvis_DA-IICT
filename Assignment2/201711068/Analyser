POST _analyze
{
  "tokenizer": "standard",
  "text": ["Hi!!! This is SampLed t3xt"]
}
output :-
{
  "tokens": [
    {
      "token": "Hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "This",
      "start_offset": 6,
      "end_offset": 10,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 11,
      "end_offset": 13,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "SampLed",
      "start_offset": 14,
      "end_offset": 21,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "t3xt",
      "start_offset": 22,
      "end_offset": 26,
      "type": "<ALPHANUM>",
      "position": 4
    }
  ]
}

PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english_analyzer": {
          "type": "standard",
          "max_token_length": 5,
          "stopwords": "_english_"
        }
      }
    }
  }
}
output :-
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "my_index"
}

DELETE my_index
output :-
{
  "acknowledged": true
}

POST my_index/_analyze
{
  "analyzer": "my_english_analyzer",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "sampl",
      "start_offset": 14,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "ed",
      "start_offset": 19,
      "end_offset": 21,
      "type": "<ALPHANUM>",
      "position": 4
    },
    {
      "token": "t3xt",
      "start_offset": 22,
      "end_offset": 26,
      "type": "<ALPHANUM>",
      "position": 5
    }
  ]
}

GET /library/_analyze
{
  "analyzer": "simple",
  "text": ["Hi!!! This is SampLed t3xt"]
}
output :-
{
  "tokens": [
    {
      "token": "hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 6,
      "end_offset": 10,
      "type": "word",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 11,
      "end_offset": 13,
      "type": "word",
      "position": 2
    },
    {
      "token": "sampled",
      "start_offset": 14,
      "end_offset": 21,
      "type": "word",
      "position": 3
    },
    {
      "token": "t",
      "start_offset": 22,
      "end_offset": 23,
      "type": "word",
      "position": 4
    },
    {
      "token": "xt",
      "start_offset": 24,
      "end_offset": 26,
      "type": "word",
      "position": 5
    }
  ]
}

POST _analyze
{
  "analyzer": "whitespace",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "Hi!!!",
      "start_offset": 0,
      "end_offset": 5,
      "type": "word",
      "position": 0
    },
    {
      "token": "This",
      "start_offset": 6,
      "end_offset": 10,
      "type": "word",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 11,
      "end_offset": 13,
      "type": "word",
      "position": 2
    },
    {
      "token": "SampLed",
      "start_offset": 14,
      "end_offset": 21,
      "type": "word",
      "position": 3
    },
    {
      "token": "t3xt",
      "start_offset": 22,
      "end_offset": 26,
      "type": "word",
      "position": 4
    }
  ]
}

POST _analyze
{
  "analyzer": "stop",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "sampled",
      "start_offset": 14,
      "end_offset": 21,
      "type": "word",
      "position": 3
    },
    {
      "token": "t",
      "start_offset": 22,
      "end_offset": 23,
      "type": "word",
      "position": 4
    },
    {
      "token": "xt",
      "start_offset": 24,
      "end_offset": 26,
      "type": "word",
      "position": 5
    }
  ]
}
DELETE my_index
output :-
{
  "acknowledged": true
}

PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_stop_analyzer": {
          "type": "stop",
          "stopwords": ["the", "over"]
        }
      }
    }
  }
}
output :-
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "my_index"
}

POST my_index/_analyze
{
  "analyzer": "my_stop_analyzer",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 6,
      "end_offset": 10,
      "type": "word",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 11,
      "end_offset": 13,
      "type": "word",
      "position": 2
    },
    {
      "token": "sampled",
      "start_offset": 14,
      "end_offset": 21,
      "type": "word",
      "position": 3
    },
    {
      "token": "t",
      "start_offset": 22,
      "end_offset": 23,
      "type": "word",
      "position": 4
    },
    {
      "token": "xt",
      "start_offset": 24,
      "end_offset": 26,
      "type": "word",
      "position": 5
    }
  ]
}

POST _analyze
{
  "analyzer": "keyword",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "Hi!!! This is SampLed t3xt",
      "start_offset": 0,
      "end_offset": 26,
      "type": "word",
      "position": 0
    }
  ]
}

POST _analyze
{
  "analyzer": "pattern",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 6,
      "end_offset": 10,
      "type": "word",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 11,
      "end_offset": 13,
      "type": "word",
      "position": 2
    },
    {
      "token": "sampled",
      "start_offset": 14,
      "end_offset": 21,
      "type": "word",
      "position": 3
    },
    {
      "token": "t3xt",
      "start_offset": 22,
      "end_offset": 26,
      "type": "word",
      "position": 4
    }
  ]
}

DELETE my_index
output :-
{
  "acknowledged": true
}

PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_email_analyzer": {
          "type":      "pattern",
          "pattern":   "\\W|_", 
          "lowercase": true
        }
      }
    }
  }
}
output :-
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "my_index"
}

POST my_index/_analyze
{
  "analyzer": "my_email_analyzer",
  "text": "krutika@gmail.com"
}
output :-
{
  "tokens": [
    {
      "token": "krutika",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "gmail",
      "start_offset": 8,
      "end_offset": 13,
      "type": "word",
      "position": 1
    },
    {
      "token": "com",
      "start_offset": 14,
      "end_offset": 17,
      "type": "word",
      "position": 2
    }
  ]
}

DELETE my_index
output :-
{
  "acknowledged": true
}

PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "camel": {
          "type": "pattern",
          "pattern": "([^\\p{L}\\d]+)|(?<=\\D)(?=\\d)|(?<=\\d)(?=\\D)|(?<=[\\p{L}&&[^\\p{Lu}]])(?=\\p{Lu})|(?<=\\p{Lu})(?=\\p{Lu}[\\p{L}&&[^\\p{Lu}]])"
        }
      }
    }
  }
}
output :-
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "my_index"
}

GET my_index/_analyze
{
  "analyzer": "camel",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 6,
      "end_offset": 10,
      "type": "word",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 11,
      "end_offset": 13,
      "type": "word",
      "position": 2
    },
    {
      "token": "samp",
      "start_offset": 14,
      "end_offset": 18,
      "type": "word",
      "position": 3
    },
    {
      "token": "led",
      "start_offset": 18,
      "end_offset": 21,
      "type": "word",
      "position": 4
    },
    {
      "token": "t",
      "start_offset": 22,
      "end_offset": 23,
      "type": "word",
      "position": 5
    },
    {
      "token": "3",
      "start_offset": 23,
      "end_offset": 24,
      "type": "word",
      "position": 6
    },
    {
      "token": "xt",
      "start_offset": 24,
      "end_offset": 26,
      "type": "word",
      "position": 7
    }
  ]
}

PUT /arabic_example
{
  "settings": {
    "analysis": {
      "filter": {
        "arabic_stop": {
          "type":       "stop",
          "stopwords":  "_arabic_" 
        },
        "arabic_keywords": {
          "type":       "keyword_marker",
          "keywords":   ["????"] 
        },
        "arabic_stemmer": {
          "type":       "stemmer",
          "language":   "arabic"
        }
      },
      "analyzer": {
        "arabic": {
          "tokenizer":  "standard",
          "filter": [
            "lowercase",
            "arabic_stop",
            "arabic_normalization",
            "arabic_keywords",
            "arabic_stemmer"
          ]
        }
      }
    }
  }
}
output :-
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "arabic_example"
}

GET arabic_example/_analyze
{
  "analyzer": "arabic",
  "text": "????? ?? ???? ?"
}
output :-
{
  "tokens": [
    {
      "token": "?????",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "????",
      "start_offset": 9,
      "end_offset": 13,
      "type": "<ALPHANUM>",
      "position": 2
    }
  ]
}

POST _analyze
{
  "analyzer": "fingerprint",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi is sampled t3xt this",
      "start_offset": 0,
      "end_offset": 26,
      "type": "fingerprint",
      "position": 0
    }
  ]
}

DELETE my_index
output :-
{
  "acknowledged": true
}

PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_fingerprint_analyzer": {
          "type": "fingerprint",
          "stopwords": "_english_"
        }
      }
    }
  }
}
output :-
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "my_index"
}

POST my_index/_analyze
{
  "analyzer": "my_fingerprint_analyzer",
  "text": "Hi!!! This is SampLed t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi sampled t3xt",
      "start_offset": 0,
      "end_offset": 26,
      "type": "fingerprint",
      "position": 0
    }
  ]
}

DELETE my_index
output :-
{
  "acknowledged": true
}

PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type":      "custom",
          "tokenizer": "standard",
          "char_filter": [
            "html_strip"
          ],
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  }
}
output :-
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "my_index"
}

POST my_index/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": "Hi!!! <b>This is SampLed</b> t3xt"
}
output :-
{
  "tokens": [
    {
      "token": "hi",
      "start_offset": 0,
      "end_offset": 2,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 9,
      "end_offset": 13,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 14,
      "end_offset": 16,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "sampled",
      "start_offset": 17,
      "end_offset": 28,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "t3xt",
      "start_offset": 29,
      "end_offset": 33,
      "type": "<ALPHANUM>",
      "position": 4
    }
  ]
}
